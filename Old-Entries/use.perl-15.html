<p>
I've been on Freenode's #perl the other day (before I had a permanent job)
when someone asked if anybody was interested in writing a new CPAN module
(to be released as open source) for a small pay. I replied that I'm interested,
and we ended up talking in PM-mode.
</p>

<p>
That module was
<a href="http://search.cpan.org/dist/WWW-Search-MSN/">WWW-Search-MSN</a>,
which is a
<a href="http://search.cpan.org/dist/WWW-Search/">WWW-Search</a> backend for
searching using <a href="http://search.msn.com/">the search.msn.com engine</a>.
It was released on CPAN, and the copyright notice says it was originally
written as a commission from
<a href="http://www.deviatemedia.com/">Deviate Media</a> and
<a href="http://www.redtreesystems.com/">Red Tree Media</a>.
</p>

<p>
After that, they asked me if I'd also like to write <a href="http://search.cpan.org/dist/WWW-Search-AOL/">WWW-Search-AOL</a> for an extra and identical fee.
So I said I would, and wrote it. I recently received both payments to my PayPal
account, so now I have a little money there. (I already used some of it to
renew my <a href="http://lwn.net/">Linux Weekly News</a> subscription.) The
fee was not too big, but enough to compensate for the time I spent on it.
(Even though unsurprisingly
<a href="http://www.dwheeler.com/sloccount/">SLOCCount</a> still claims they
would have cost more for a software house to develop).
</p>

<p>
Now for some insights about the code itself. I got the HTML parsed and could
process it using
<a href="http://search.cpan.org/dist/HTML-Tree/">HTML-Tree</a>. The standard
methods (<tt>look_up</tt>, <tt>look_down</tt>, etc.) were relatively convenient
and I could whip up a working version pretty quickly. Only problem is that
later on it broke on various edge cases, when the HTML returned by the
search engines was somewhat differnet (like when no results are returned). I
have been too lazy to catch all undef returns from the HTML-Tree calls, and so
the code often breaks on such cases.
</p>

<p>
There are already <a href="http://rt.cpan.org/Public/Dist/Display.html?Name=WWW-Search-MSN">two bugs for WWW-Search-MSN</a>, one of them about the "Encode"
dependency (which I didn't realise entered the core only after 5.6.1), and
one about it dying on invalid input. MSN has a SOAP service for accessing
results of their search, but SOAP is scary, and SOAP::Lite much more so. The
guy who commissioned me to write the module tried to write something for that,
and failed miserably. I do wish there was something simpler than SOAP for it
like well-formed YAML or JSON input, which would have been a cake to implement
in Perl. XML-RPC would also probably be better than SOAP for that.
</p>

<p>
As for the HTML - the search.msn.com HTML seemed very clean (at least after
passing through HTML tidy) and was easy to analyse. search.aol.com seems like
a Web 2.0-ish skin for Google, which was harder to analyse, but also not
impossible. It might have a web service too.
</p>

<p>
While we're on the subject of getting some perks, someone I know online
bought me
<a href="http://www.amazon.com/exec/obidos/ASIN/0596001738/ref=nosim/shlomifishhom-20">"Perl
    Best Practices" by Damian Conway</a> and <a href="http://www.amazon.com/exec/obidos/ASIN/0345348036/ref=nosim/shlomifishhom-20">"The
    Princess Bride" (The Book)</a> from my Amazon.com wishlist. I've already
started reading "Perl Best Practices" before I'm going to sleep. So far, the
best practices contain useful and thought-provoking advice, even if I sometimes
have my reasons for disagreeing with them, or thinking otherwise.
</p>

